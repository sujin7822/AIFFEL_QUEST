{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425ae157",
   "metadata": {},
   "source": [
    "## (1) 데이터 가져오기\n",
    "sklearn.datasets 의 load_diabetes 에서 데이터를 가져와주세요.\n",
    "diabetes 의 data 를 df_X 에, target 을 df_y 에 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d62fb296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "dataset = sklearn.datasets.load_diabetes()\n",
    "\n",
    "df_x=dataset.data\n",
    "df_y=dataset.target\n",
    "\n",
    "print(df_x.shape)\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d7d6c",
   "metadata": {},
   "source": [
    "## (2) 모델에 입력할 데이터 X 준비하기\n",
    "df_X 에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c398f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_X=df_x\n",
    "#df_X = np.array(df_x)\n",
    "print(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b361a849",
   "metadata": {},
   "source": [
    "## (3) 모델에 예측할 데이터 y 준비하기\n",
    "df_y 에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f9bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "#df_y = np.array(df_y)\n",
    "print(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9d02e",
   "metadata": {},
   "source": [
    "## (4) train 데이터와 test 데이터로 분리하기\n",
    "X 와 y  데이터를 각각 train 데이터와 test 데이터로 분리해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce8e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ae40e",
   "metadata": {},
   "source": [
    "## (5) 모델 준비하기\n",
    "입력 데이터 개수에 맞는 가중치 W 와 b 를 준비해주세요.\n",
    "모델 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd0fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    #for i in range(10):\n",
    "    num_features = X.shape[1]\n",
    "    for i in range(num_features):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18849589",
   "metadata": {},
   "source": [
    "## (6) 손실함수 loss 정의하기\n",
    "손실함수를 MSE 함수로 정의해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf3e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "  mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "  return mse\n",
    "\n",
    "def loss(x, w, b, y):\n",
    "  predictions = model(x, w, b)\n",
    "  L = MSE(predictions, y)\n",
    "  return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bb8c8",
   "metadata": {},
   "source": [
    "## (7) 기울기를 구하는 gradient 함수 구현하기\n",
    "기울기를 계산하는 gradient  함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eb2ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86868ef",
   "metadata": {},
   "source": [
    "## (8) 하이퍼 파라미터인 학습률 설정하기\n",
    "학습률, learning rate 를 설정해주세요\n",
    "만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a1b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597da1ca",
   "metadata": {},
   "source": [
    "## (9) 모델 학습하기\n",
    "정의된 손실함수와 기울기 함수로 모델을 학습해주세요.\n",
    "loss값이 충분히 떨어질 때까지 학습을 진행해주세요.\n",
    "입력하는 데이터인 X 에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba85049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_ = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c4f31f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 6326.6089\n",
      "Iteration 20 : Loss 5981.6766\n",
      "Iteration 30 : Loss 5904.5412\n",
      "Iteration 40 : Loss 5832.7676\n",
      "Iteration 50 : Loss 5763.2528\n",
      "Iteration 60 : Loss 5695.8862\n",
      "Iteration 70 : Loss 5630.5942\n",
      "Iteration 80 : Loss 5567.3062\n",
      "Iteration 90 : Loss 5505.9541\n",
      "Iteration 100 : Loss 5446.4721\n",
      "Iteration 110 : Loss 5388.7969\n",
      "Iteration 120 : Loss 5332.8673\n",
      "Iteration 130 : Loss 5278.6243\n",
      "Iteration 140 : Loss 5226.0108\n",
      "Iteration 150 : Loss 5174.9719\n",
      "Iteration 160 : Loss 5125.4546\n",
      "Iteration 170 : Loss 5077.4078\n",
      "Iteration 180 : Loss 5030.7820\n",
      "Iteration 190 : Loss 4985.5295\n",
      "Iteration 200 : Loss 4941.6045\n",
      "Iteration 210 : Loss 4898.9624\n",
      "Iteration 220 : Loss 4857.5606\n",
      "Iteration 230 : Loss 4817.3576\n",
      "Iteration 240 : Loss 4778.3136\n",
      "Iteration 250 : Loss 4740.3901\n",
      "Iteration 260 : Loss 4703.5500\n",
      "Iteration 270 : Loss 4667.7573\n",
      "Iteration 280 : Loss 4632.9775\n",
      "Iteration 290 : Loss 4599.1772\n",
      "Iteration 300 : Loss 4566.3242\n",
      "Iteration 310 : Loss 4534.3872\n",
      "Iteration 320 : Loss 4503.3364\n",
      "Iteration 330 : Loss 4473.1427\n",
      "Iteration 340 : Loss 4443.7782\n",
      "Iteration 350 : Loss 4415.2157\n",
      "Iteration 360 : Loss 4387.4293\n",
      "Iteration 370 : Loss 4360.3938\n",
      "Iteration 380 : Loss 4334.0848\n",
      "Iteration 390 : Loss 4308.4790\n",
      "Iteration 400 : Loss 4283.5536\n",
      "Iteration 410 : Loss 4259.2867\n",
      "Iteration 420 : Loss 4235.6574\n",
      "Iteration 430 : Loss 4212.6452\n",
      "Iteration 440 : Loss 4190.2303\n",
      "Iteration 450 : Loss 4168.3939\n",
      "Iteration 460 : Loss 4147.1175\n",
      "Iteration 470 : Loss 4126.3835\n",
      "Iteration 480 : Loss 4106.1747\n",
      "Iteration 490 : Loss 4086.4745\n",
      "Iteration 500 : Loss 4067.2671\n",
      "Iteration 510 : Loss 4048.5370\n",
      "Iteration 520 : Loss 4030.2694\n",
      "Iteration 530 : Loss 4012.4498\n",
      "Iteration 540 : Loss 3995.0643\n",
      "Iteration 550 : Loss 3978.0996\n",
      "Iteration 560 : Loss 3961.5427\n",
      "Iteration 570 : Loss 3945.3811\n",
      "Iteration 580 : Loss 3929.6026\n",
      "Iteration 590 : Loss 3914.1957\n",
      "Iteration 600 : Loss 3899.1489\n",
      "Iteration 610 : Loss 3884.4514\n",
      "Iteration 620 : Loss 3870.0927\n",
      "Iteration 630 : Loss 3856.0626\n",
      "Iteration 640 : Loss 3842.3512\n",
      "Iteration 650 : Loss 3828.9491\n",
      "Iteration 660 : Loss 3815.8470\n",
      "Iteration 670 : Loss 3803.0360\n",
      "Iteration 680 : Loss 3790.5077\n",
      "Iteration 690 : Loss 3778.2536\n",
      "Iteration 700 : Loss 3766.2658\n",
      "Iteration 710 : Loss 3754.5366\n",
      "Iteration 720 : Loss 3743.0583\n",
      "Iteration 730 : Loss 3731.8238\n",
      "Iteration 740 : Loss 3720.8261\n",
      "Iteration 750 : Loss 3710.0584\n",
      "Iteration 760 : Loss 3699.5142\n",
      "Iteration 770 : Loss 3689.1871\n",
      "Iteration 780 : Loss 3679.0710\n",
      "Iteration 790 : Loss 3669.1599\n",
      "Iteration 800 : Loss 3659.4482\n",
      "Iteration 810 : Loss 3649.9303\n",
      "Iteration 820 : Loss 3640.6009\n",
      "Iteration 830 : Loss 3631.4547\n",
      "Iteration 840 : Loss 3622.4868\n",
      "Iteration 850 : Loss 3613.6922\n",
      "Iteration 860 : Loss 3605.0664\n",
      "Iteration 870 : Loss 3596.6046\n",
      "Iteration 880 : Loss 3588.3027\n",
      "Iteration 890 : Loss 3580.1562\n",
      "Iteration 900 : Loss 3572.1612\n",
      "Iteration 910 : Loss 3564.3135\n",
      "Iteration 920 : Loss 3556.6093\n",
      "Iteration 930 : Loss 3549.0449\n",
      "Iteration 940 : Loss 3541.6167\n",
      "Iteration 950 : Loss 3534.3212\n",
      "Iteration 960 : Loss 3527.1549\n",
      "Iteration 970 : Loss 3520.1147\n",
      "Iteration 980 : Loss 3513.1972\n",
      "Iteration 990 : Loss 3506.3994\n",
      "Iteration 1000 : Loss 3499.7184\n",
      "Iteration 1010 : Loss 3493.1512\n",
      "Iteration 1020 : Loss 3486.6950\n",
      "Iteration 1030 : Loss 3480.3472\n",
      "Iteration 1040 : Loss 3474.1050\n",
      "Iteration 1050 : Loss 3467.9660\n",
      "Iteration 1060 : Loss 3461.9276\n",
      "Iteration 1070 : Loss 3455.9875\n",
      "Iteration 1080 : Loss 3450.1433\n",
      "Iteration 1090 : Loss 3444.3927\n",
      "Iteration 1100 : Loss 3438.7337\n",
      "Iteration 1110 : Loss 3433.1640\n",
      "Iteration 1120 : Loss 3427.6816\n",
      "Iteration 1130 : Loss 3422.2846\n",
      "Iteration 1140 : Loss 3416.9709\n",
      "Iteration 1150 : Loss 3411.7387\n",
      "Iteration 1160 : Loss 3406.5861\n",
      "Iteration 1170 : Loss 3401.5115\n",
      "Iteration 1180 : Loss 3396.5131\n",
      "Iteration 1190 : Loss 3391.5891\n",
      "Iteration 1200 : Loss 3386.7381\n",
      "Iteration 1210 : Loss 3381.9584\n",
      "Iteration 1220 : Loss 3377.2484\n",
      "Iteration 1230 : Loss 3372.6068\n",
      "Iteration 1240 : Loss 3368.0321\n",
      "Iteration 1250 : Loss 3363.5228\n",
      "Iteration 1260 : Loss 3359.0776\n",
      "Iteration 1270 : Loss 3354.6953\n",
      "Iteration 1280 : Loss 3350.3744\n",
      "Iteration 1290 : Loss 3346.1138\n",
      "Iteration 1300 : Loss 3341.9122\n",
      "Iteration 1310 : Loss 3337.7685\n",
      "Iteration 1320 : Loss 3333.6815\n",
      "Iteration 1330 : Loss 3329.6500\n",
      "Iteration 1340 : Loss 3325.6731\n",
      "Iteration 1350 : Loss 3321.7497\n",
      "Iteration 1360 : Loss 3317.8786\n",
      "Iteration 1370 : Loss 3314.0590\n",
      "Iteration 1380 : Loss 3310.2898\n",
      "Iteration 1390 : Loss 3306.5701\n",
      "Iteration 1400 : Loss 3302.8990\n",
      "Iteration 1410 : Loss 3299.2756\n",
      "Iteration 1420 : Loss 3295.6989\n",
      "Iteration 1430 : Loss 3292.1682\n",
      "Iteration 1440 : Loss 3288.6826\n",
      "Iteration 1450 : Loss 3285.2413\n",
      "Iteration 1460 : Loss 3281.8435\n",
      "Iteration 1470 : Loss 3278.4885\n",
      "Iteration 1480 : Loss 3275.1754\n",
      "Iteration 1490 : Loss 3271.9036\n",
      "Iteration 1500 : Loss 3268.6724\n",
      "Iteration 1510 : Loss 3265.4810\n",
      "Iteration 1520 : Loss 3262.3287\n",
      "Iteration 1530 : Loss 3259.2150\n",
      "Iteration 1540 : Loss 3256.1392\n",
      "Iteration 1550 : Loss 3253.1005\n",
      "Iteration 1560 : Loss 3250.0985\n",
      "Iteration 1570 : Loss 3247.1325\n",
      "Iteration 1580 : Loss 3244.2020\n",
      "Iteration 1590 : Loss 3241.3063\n",
      "Iteration 1600 : Loss 3238.4448\n",
      "Iteration 1610 : Loss 3235.6172\n",
      "Iteration 1620 : Loss 3232.8227\n",
      "Iteration 1630 : Loss 3230.0609\n",
      "Iteration 1640 : Loss 3227.3314\n",
      "Iteration 1650 : Loss 3224.6335\n",
      "Iteration 1660 : Loss 3221.9667\n",
      "Iteration 1670 : Loss 3219.3307\n",
      "Iteration 1680 : Loss 3216.7250\n",
      "Iteration 1690 : Loss 3214.1491\n",
      "Iteration 1700 : Loss 3211.6024\n",
      "Iteration 1710 : Loss 3209.0848\n",
      "Iteration 1720 : Loss 3206.5955\n",
      "Iteration 1730 : Loss 3204.1344\n",
      "Iteration 1740 : Loss 3201.7009\n",
      "Iteration 1750 : Loss 3199.2946\n",
      "Iteration 1760 : Loss 3196.9153\n",
      "Iteration 1770 : Loss 3194.5623\n",
      "Iteration 1780 : Loss 3192.2355\n",
      "Iteration 1790 : Loss 3189.9344\n",
      "Iteration 1800 : Loss 3187.6587\n",
      "Iteration 1810 : Loss 3185.4080\n",
      "Iteration 1820 : Loss 3183.1819\n",
      "Iteration 1830 : Loss 3180.9802\n",
      "Iteration 1840 : Loss 3178.8025\n",
      "Iteration 1850 : Loss 3176.6484\n",
      "Iteration 1860 : Loss 3174.5177\n",
      "Iteration 1870 : Loss 3172.4099\n",
      "Iteration 1880 : Loss 3170.3249\n",
      "Iteration 1890 : Loss 3168.2623\n",
      "Iteration 1900 : Loss 3166.2219\n",
      "Iteration 1910 : Loss 3164.2032\n",
      "Iteration 1920 : Loss 3162.2061\n",
      "Iteration 1930 : Loss 3160.2302\n",
      "Iteration 1940 : Loss 3158.2753\n",
      "Iteration 1950 : Loss 3156.3411\n",
      "Iteration 1960 : Loss 3154.4273\n",
      "Iteration 1970 : Loss 3152.5337\n",
      "Iteration 1980 : Loss 3150.6600\n",
      "Iteration 1990 : Loss 3148.8060\n",
      "Iteration 2000 : Loss 3146.9714\n",
      "Iteration 2010 : Loss 3145.1560\n",
      "Iteration 2020 : Loss 3143.3595\n",
      "Iteration 2030 : Loss 3141.5817\n",
      "Iteration 2040 : Loss 3139.8223\n",
      "Iteration 2050 : Loss 3138.0812\n",
      "Iteration 2060 : Loss 3136.3581\n",
      "Iteration 2070 : Loss 3134.6528\n",
      "Iteration 2080 : Loss 3132.9651\n",
      "Iteration 2090 : Loss 3131.2947\n",
      "Iteration 2100 : Loss 3129.6414\n",
      "Iteration 2110 : Loss 3128.0051\n",
      "Iteration 2120 : Loss 3126.3856\n",
      "Iteration 2130 : Loss 3124.7825\n",
      "Iteration 2140 : Loss 3123.1958\n",
      "Iteration 2150 : Loss 3121.6253\n",
      "Iteration 2160 : Loss 3120.0707\n",
      "Iteration 2170 : Loss 3118.5319\n",
      "Iteration 2180 : Loss 3117.0086\n",
      "Iteration 2190 : Loss 3115.5008\n",
      "Iteration 2200 : Loss 3114.0081\n",
      "Iteration 2210 : Loss 3112.5305\n",
      "Iteration 2220 : Loss 3111.0678\n",
      "Iteration 2230 : Loss 3109.6198\n",
      "Iteration 2240 : Loss 3108.1862\n",
      "Iteration 2250 : Loss 3106.7671\n",
      "Iteration 2260 : Loss 3105.3621\n",
      "Iteration 2270 : Loss 3103.9712\n",
      "Iteration 2280 : Loss 3102.5941\n",
      "Iteration 2290 : Loss 3101.2308\n",
      "Iteration 2300 : Loss 3099.8810\n",
      "Iteration 2310 : Loss 3098.5446\n",
      "Iteration 2320 : Loss 3097.2215\n",
      "Iteration 2330 : Loss 3095.9115\n",
      "Iteration 2340 : Loss 3094.6145\n",
      "Iteration 2350 : Loss 3093.3302\n",
      "Iteration 2360 : Loss 3092.0586\n",
      "Iteration 2370 : Loss 3090.7996\n",
      "Iteration 2380 : Loss 3089.5530\n",
      "Iteration 2390 : Loss 3088.3186\n",
      "Iteration 2400 : Loss 3087.0963\n",
      "Iteration 2410 : Loss 3085.8860\n",
      "Iteration 2420 : Loss 3084.6876\n",
      "Iteration 2430 : Loss 3083.5009\n",
      "Iteration 2440 : Loss 3082.3258\n",
      "Iteration 2450 : Loss 3081.1622\n",
      "Iteration 2460 : Loss 3080.0099\n",
      "Iteration 2470 : Loss 3078.8689\n",
      "Iteration 2480 : Loss 3077.7390\n",
      "Iteration 2490 : Loss 3076.6200\n",
      "Iteration 2500 : Loss 3075.5119\n",
      "Iteration 2510 : Loss 3074.4145\n",
      "Iteration 2520 : Loss 3073.3278\n",
      "Iteration 2530 : Loss 3072.2516\n",
      "Iteration 2540 : Loss 3071.1858\n",
      "Iteration 2550 : Loss 3070.1303\n",
      "Iteration 2560 : Loss 3069.0850\n",
      "Iteration 2570 : Loss 3068.0498\n",
      "Iteration 2580 : Loss 3067.0245\n",
      "Iteration 2590 : Loss 3066.0091\n",
      "Iteration 2600 : Loss 3065.0035\n",
      "Iteration 2610 : Loss 3064.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2620 : Loss 3063.0210\n",
      "Iteration 2630 : Loss 3062.0441\n",
      "Iteration 2640 : Loss 3061.0764\n",
      "Iteration 2650 : Loss 3060.1181\n",
      "Iteration 2660 : Loss 3059.1689\n",
      "Iteration 2670 : Loss 3058.2287\n",
      "Iteration 2680 : Loss 3057.2975\n",
      "Iteration 2690 : Loss 3056.3753\n",
      "Iteration 2700 : Loss 3055.4617\n",
      "Iteration 2710 : Loss 3054.5569\n",
      "Iteration 2720 : Loss 3053.6607\n",
      "Iteration 2730 : Loss 3052.7730\n",
      "Iteration 2740 : Loss 3051.8937\n",
      "Iteration 2750 : Loss 3051.0228\n",
      "Iteration 2760 : Loss 3050.1601\n",
      "Iteration 2770 : Loss 3049.3055\n",
      "Iteration 2780 : Loss 3048.4591\n",
      "Iteration 2790 : Loss 3047.6206\n",
      "Iteration 2800 : Loss 3046.7901\n",
      "Iteration 2810 : Loss 3045.9674\n",
      "Iteration 2820 : Loss 3045.1525\n",
      "Iteration 2830 : Loss 3044.3452\n",
      "Iteration 2840 : Loss 3043.5455\n",
      "Iteration 2850 : Loss 3042.7534\n",
      "Iteration 2860 : Loss 3041.9687\n",
      "Iteration 2870 : Loss 3041.1914\n",
      "Iteration 2880 : Loss 3040.4213\n",
      "Iteration 2890 : Loss 3039.6585\n",
      "Iteration 2900 : Loss 3038.9028\n",
      "Iteration 2910 : Loss 3038.1542\n",
      "Iteration 2920 : Loss 3037.4126\n",
      "Iteration 2930 : Loss 3036.6779\n",
      "Iteration 2940 : Loss 3035.9501\n",
      "Iteration 2950 : Loss 3035.2291\n",
      "Iteration 2960 : Loss 3034.5148\n",
      "Iteration 2970 : Loss 3033.8071\n",
      "Iteration 2980 : Loss 3033.1061\n",
      "Iteration 2990 : Loss 3032.4115\n",
      "Iteration 3000 : Loss 3031.7234\n",
      "Iteration 3010 : Loss 3031.0417\n",
      "Iteration 3020 : Loss 3030.3664\n",
      "Iteration 3030 : Loss 3029.6973\n",
      "Iteration 3040 : Loss 3029.0343\n",
      "Iteration 3050 : Loss 3028.3776\n",
      "Iteration 3060 : Loss 3027.7269\n",
      "Iteration 3070 : Loss 3027.0822\n",
      "Iteration 3080 : Loss 3026.4435\n",
      "Iteration 3090 : Loss 3025.8107\n",
      "Iteration 3100 : Loss 3025.1837\n",
      "Iteration 3110 : Loss 3024.5625\n",
      "Iteration 3120 : Loss 3023.9470\n",
      "Iteration 3130 : Loss 3023.3372\n",
      "Iteration 3140 : Loss 3022.7330\n",
      "Iteration 3150 : Loss 3022.1344\n",
      "Iteration 3160 : Loss 3021.5412\n",
      "Iteration 3170 : Loss 3020.9535\n",
      "Iteration 3180 : Loss 3020.3712\n",
      "Iteration 3190 : Loss 3019.7942\n",
      "Iteration 3200 : Loss 3019.2225\n",
      "Iteration 3210 : Loss 3018.6561\n",
      "Iteration 3220 : Loss 3018.0948\n",
      "Iteration 3230 : Loss 3017.5386\n",
      "Iteration 3240 : Loss 3016.9876\n",
      "Iteration 3250 : Loss 3016.4415\n",
      "Iteration 3260 : Loss 3015.9005\n",
      "Iteration 3270 : Loss 3015.3644\n",
      "Iteration 3280 : Loss 3014.8331\n",
      "Iteration 3290 : Loss 3014.3067\n",
      "Iteration 3300 : Loss 3013.7851\n",
      "Iteration 3310 : Loss 3013.2682\n",
      "Iteration 3320 : Loss 3012.7560\n",
      "Iteration 3330 : Loss 3012.2485\n",
      "Iteration 3340 : Loss 3011.7456\n",
      "Iteration 3350 : Loss 3011.2472\n",
      "Iteration 3360 : Loss 3010.7533\n",
      "Iteration 3370 : Loss 3010.2639\n",
      "Iteration 3380 : Loss 3009.7790\n",
      "Iteration 3390 : Loss 3009.2984\n",
      "Iteration 3400 : Loss 3008.8221\n",
      "Iteration 3410 : Loss 3008.3502\n",
      "Iteration 3420 : Loss 3007.8825\n",
      "Iteration 3430 : Loss 3007.4190\n",
      "Iteration 3440 : Loss 3006.9597\n",
      "Iteration 3450 : Loss 3006.5046\n",
      "Iteration 3460 : Loss 3006.0535\n",
      "Iteration 3470 : Loss 3005.6065\n",
      "Iteration 3480 : Loss 3005.1635\n",
      "Iteration 3490 : Loss 3004.7244\n",
      "Iteration 3500 : Loss 3004.2894\n",
      "Iteration 3510 : Loss 3003.8582\n",
      "Iteration 3520 : Loss 3003.4308\n",
      "Iteration 3530 : Loss 3003.0073\n",
      "Iteration 3540 : Loss 3002.5876\n",
      "Iteration 3550 : Loss 3002.1717\n",
      "Iteration 3560 : Loss 3001.7594\n",
      "Iteration 3570 : Loss 3001.3508\n",
      "Iteration 3580 : Loss 3000.9459\n",
      "Iteration 3590 : Loss 3000.5446\n",
      "Iteration 3600 : Loss 3000.1468\n",
      "Iteration 3610 : Loss 2999.7526\n",
      "Iteration 3620 : Loss 2999.3619\n",
      "Iteration 3630 : Loss 2998.9747\n",
      "Iteration 3640 : Loss 2998.5909\n",
      "Iteration 3650 : Loss 2998.2105\n",
      "Iteration 3660 : Loss 2997.8335\n",
      "Iteration 3670 : Loss 2997.4599\n",
      "Iteration 3680 : Loss 2997.0895\n",
      "Iteration 3690 : Loss 2996.7224\n",
      "Iteration 3700 : Loss 2996.3586\n",
      "Iteration 3710 : Loss 2995.9980\n",
      "Iteration 3720 : Loss 2995.6406\n",
      "Iteration 3730 : Loss 2995.2863\n",
      "Iteration 3740 : Loss 2994.9351\n",
      "Iteration 3750 : Loss 2994.5871\n",
      "Iteration 3760 : Loss 2994.2421\n",
      "Iteration 3770 : Loss 2993.9001\n",
      "Iteration 3780 : Loss 2993.5612\n",
      "Iteration 3790 : Loss 2993.2252\n",
      "Iteration 3800 : Loss 2992.8922\n",
      "Iteration 3810 : Loss 2992.5621\n",
      "Iteration 3820 : Loss 2992.2349\n",
      "Iteration 3830 : Loss 2991.9106\n",
      "Iteration 3840 : Loss 2991.5891\n",
      "Iteration 3850 : Loss 2991.2704\n",
      "Iteration 3860 : Loss 2990.9545\n",
      "Iteration 3870 : Loss 2990.6413\n",
      "Iteration 3880 : Loss 2990.3310\n",
      "Iteration 3890 : Loss 2990.0233\n",
      "Iteration 3900 : Loss 2989.7183\n",
      "Iteration 3910 : Loss 2989.4159\n",
      "Iteration 3920 : Loss 2989.1162\n",
      "Iteration 3930 : Loss 2988.8191\n",
      "Iteration 3940 : Loss 2988.5246\n",
      "Iteration 3950 : Loss 2988.2326\n",
      "Iteration 3960 : Loss 2987.9432\n",
      "Iteration 3970 : Loss 2987.6562\n",
      "Iteration 3980 : Loss 2987.3718\n",
      "Iteration 3990 : Loss 2987.0898\n",
      "Iteration 4000 : Loss 2986.8103\n",
      "Iteration 4010 : Loss 2986.5332\n",
      "Iteration 4020 : Loss 2986.2585\n",
      "Iteration 4030 : Loss 2985.9862\n",
      "Iteration 4040 : Loss 2985.7162\n",
      "Iteration 4050 : Loss 2985.4485\n",
      "Iteration 4060 : Loss 2985.1832\n",
      "Iteration 4070 : Loss 2984.9201\n",
      "Iteration 4080 : Loss 2984.6593\n",
      "Iteration 4090 : Loss 2984.4008\n",
      "Iteration 4100 : Loss 2984.1444\n",
      "Iteration 4110 : Loss 2983.8903\n",
      "Iteration 4120 : Loss 2983.6383\n",
      "Iteration 4130 : Loss 2983.3885\n",
      "Iteration 4140 : Loss 2983.1409\n",
      "Iteration 4150 : Loss 2982.8954\n",
      "Iteration 4160 : Loss 2982.6519\n",
      "Iteration 4170 : Loss 2982.4106\n",
      "Iteration 4180 : Loss 2982.1713\n",
      "Iteration 4190 : Loss 2981.9341\n",
      "Iteration 4200 : Loss 2981.6989\n",
      "Iteration 4210 : Loss 2981.4657\n",
      "Iteration 4220 : Loss 2981.2344\n",
      "Iteration 4230 : Loss 2981.0052\n",
      "Iteration 4240 : Loss 2980.7779\n",
      "Iteration 4250 : Loss 2980.5525\n",
      "Iteration 4260 : Loss 2980.3291\n",
      "Iteration 4270 : Loss 2980.1075\n",
      "Iteration 4280 : Loss 2979.8878\n",
      "Iteration 4290 : Loss 2979.6700\n",
      "Iteration 4300 : Loss 2979.4540\n",
      "Iteration 4310 : Loss 2979.2399\n",
      "Iteration 4320 : Loss 2979.0276\n",
      "Iteration 4330 : Loss 2978.8170\n",
      "Iteration 4340 : Loss 2978.6083\n",
      "Iteration 4350 : Loss 2978.4013\n",
      "Iteration 4360 : Loss 2978.1960\n",
      "Iteration 4370 : Loss 2977.9925\n",
      "Iteration 4380 : Loss 2977.7907\n",
      "Iteration 4390 : Loss 2977.5906\n",
      "Iteration 4400 : Loss 2977.3921\n",
      "Iteration 4410 : Loss 2977.1954\n",
      "Iteration 4420 : Loss 2977.0003\n",
      "Iteration 4430 : Loss 2976.8068\n",
      "Iteration 4440 : Loss 2976.6149\n",
      "Iteration 4450 : Loss 2976.4247\n",
      "Iteration 4460 : Loss 2976.2360\n",
      "Iteration 4470 : Loss 2976.0490\n",
      "Iteration 4480 : Loss 2975.8635\n",
      "Iteration 4490 : Loss 2975.6795\n",
      "Iteration 4500 : Loss 2975.4971\n",
      "Iteration 4510 : Loss 2975.3162\n",
      "Iteration 4520 : Loss 2975.1368\n",
      "Iteration 4530 : Loss 2974.9588\n",
      "Iteration 4540 : Loss 2974.7824\n",
      "Iteration 4550 : Loss 2974.6075\n",
      "Iteration 4560 : Loss 2974.4340\n",
      "Iteration 4570 : Loss 2974.2619\n",
      "Iteration 4580 : Loss 2974.0912\n",
      "Iteration 4590 : Loss 2973.9220\n",
      "Iteration 4600 : Loss 2973.7542\n",
      "Iteration 4610 : Loss 2973.5878\n",
      "Iteration 4620 : Loss 2973.4227\n",
      "Iteration 4630 : Loss 2973.2590\n",
      "Iteration 4640 : Loss 2973.0966\n",
      "Iteration 4650 : Loss 2972.9356\n",
      "Iteration 4660 : Loss 2972.7760\n",
      "Iteration 4670 : Loss 2972.6176\n",
      "Iteration 4680 : Loss 2972.4605\n",
      "Iteration 4690 : Loss 2972.3048\n",
      "Iteration 4700 : Loss 2972.1503\n",
      "Iteration 4710 : Loss 2971.9970\n",
      "Iteration 4720 : Loss 2971.8451\n",
      "Iteration 4730 : Loss 2971.6943\n",
      "Iteration 4740 : Loss 2971.5449\n",
      "Iteration 4750 : Loss 2971.3966\n",
      "Iteration 4760 : Loss 2971.2495\n",
      "Iteration 4770 : Loss 2971.1037\n",
      "Iteration 4780 : Loss 2970.9590\n",
      "Iteration 4790 : Loss 2970.8155\n",
      "Iteration 4800 : Loss 2970.6732\n",
      "Iteration 4810 : Loss 2970.5321\n",
      "Iteration 4820 : Loss 2970.3921\n",
      "Iteration 4830 : Loss 2970.2532\n",
      "Iteration 4840 : Loss 2970.1155\n",
      "Iteration 4850 : Loss 2969.9789\n",
      "Iteration 4860 : Loss 2969.8433\n",
      "Iteration 4870 : Loss 2969.7089\n",
      "Iteration 4880 : Loss 2969.5756\n",
      "Iteration 4890 : Loss 2969.4434\n",
      "Iteration 4900 : Loss 2969.3122\n",
      "Iteration 4910 : Loss 2969.1821\n",
      "Iteration 4920 : Loss 2969.0530\n",
      "Iteration 4930 : Loss 2968.9250\n",
      "Iteration 4940 : Loss 2968.7980\n",
      "Iteration 4950 : Loss 2968.6720\n",
      "Iteration 4960 : Loss 2968.5470\n",
      "Iteration 4970 : Loss 2968.4231\n",
      "Iteration 4980 : Loss 2968.3001\n",
      "Iteration 4990 : Loss 2968.1781\n",
      "Iteration 5000 : Loss 2968.0572\n",
      "Iteration 5010 : Loss 2967.9371\n",
      "Iteration 5020 : Loss 2967.8181\n",
      "Iteration 5030 : Loss 2967.6999\n",
      "Iteration 5040 : Loss 2967.5828\n",
      "Iteration 5050 : Loss 2967.4665\n",
      "Iteration 5060 : Loss 2967.3512\n",
      "Iteration 5070 : Loss 2967.2369\n",
      "Iteration 5080 : Loss 2967.1234\n",
      "Iteration 5090 : Loss 2967.0108\n",
      "Iteration 5100 : Loss 2966.8991\n",
      "Iteration 5110 : Loss 2966.7883\n",
      "Iteration 5120 : Loss 2966.6784\n",
      "Iteration 5130 : Loss 2966.5694\n",
      "Iteration 5140 : Loss 2966.4612\n",
      "Iteration 5150 : Loss 2966.3539\n",
      "Iteration 5160 : Loss 2966.2474\n",
      "Iteration 5170 : Loss 2966.1418\n",
      "Iteration 5180 : Loss 2966.0370\n",
      "Iteration 5190 : Loss 2965.9330\n",
      "Iteration 5200 : Loss 2965.8299\n",
      "Iteration 5210 : Loss 2965.7276\n",
      "Iteration 5220 : Loss 2965.6260\n",
      "Iteration 5230 : Loss 2965.5253\n",
      "Iteration 5240 : Loss 2965.4254\n",
      "Iteration 5250 : Loss 2965.3262\n",
      "Iteration 5260 : Loss 2965.2278\n",
      "Iteration 5270 : Loss 2965.1302\n",
      "Iteration 5280 : Loss 2965.0334\n",
      "Iteration 5290 : Loss 2964.9373\n",
      "Iteration 5300 : Loss 2964.8420\n",
      "Iteration 5310 : Loss 2964.7474\n",
      "Iteration 5320 : Loss 2964.6535\n",
      "Iteration 5330 : Loss 2964.5604\n",
      "Iteration 5340 : Loss 2964.4680\n",
      "Iteration 5350 : Loss 2964.3763\n",
      "Iteration 5360 : Loss 2964.2854\n",
      "Iteration 5370 : Loss 2964.1951\n",
      "Iteration 5380 : Loss 2964.1056\n",
      "Iteration 5390 : Loss 2964.0167\n",
      "Iteration 5400 : Loss 2963.9285\n",
      "Iteration 5410 : Loss 2963.8411\n",
      "Iteration 5420 : Loss 2963.7542\n",
      "Iteration 5430 : Loss 2963.6681\n",
      "Iteration 5440 : Loss 2963.5826\n",
      "Iteration 5450 : Loss 2963.4978\n",
      "Iteration 5460 : Loss 2963.4136\n",
      "Iteration 5470 : Loss 2963.3301\n",
      "Iteration 5480 : Loss 2963.2473\n",
      "Iteration 5490 : Loss 2963.1650\n",
      "Iteration 5500 : Loss 2963.0834\n",
      "Iteration 5510 : Loss 2963.0024\n",
      "Iteration 5520 : Loss 2962.9221\n",
      "Iteration 5530 : Loss 2962.8423\n",
      "Iteration 5540 : Loss 2962.7632\n",
      "Iteration 5550 : Loss 2962.6847\n",
      "Iteration 5560 : Loss 2962.6068\n",
      "Iteration 5570 : Loss 2962.5294\n",
      "Iteration 5580 : Loss 2962.4527\n",
      "Iteration 5590 : Loss 2962.3765\n",
      "Iteration 5600 : Loss 2962.3009\n",
      "Iteration 5610 : Loss 2962.2259\n",
      "Iteration 5620 : Loss 2962.1515\n",
      "Iteration 5630 : Loss 2962.0776\n",
      "Iteration 5640 : Loss 2962.0043\n",
      "Iteration 5650 : Loss 2961.9316\n",
      "Iteration 5660 : Loss 2961.8594\n",
      "Iteration 5670 : Loss 2961.7877\n",
      "Iteration 5680 : Loss 2961.7166\n",
      "Iteration 5690 : Loss 2961.6460\n",
      "Iteration 5700 : Loss 2961.5760\n",
      "Iteration 5710 : Loss 2961.5064\n",
      "Iteration 5720 : Loss 2961.4374\n",
      "Iteration 5730 : Loss 2961.3690\n",
      "Iteration 5740 : Loss 2961.3010\n",
      "Iteration 5750 : Loss 2961.2335\n",
      "Iteration 5760 : Loss 2961.1666\n",
      "Iteration 5770 : Loss 2961.1002\n",
      "Iteration 5780 : Loss 2961.0342\n",
      "Iteration 5790 : Loss 2960.9687\n",
      "Iteration 5800 : Loss 2960.9038\n",
      "Iteration 5810 : Loss 2960.8393\n",
      "Iteration 5820 : Loss 2960.7753\n",
      "Iteration 5830 : Loss 2960.7118\n",
      "Iteration 5840 : Loss 2960.6487\n",
      "Iteration 5850 : Loss 2960.5861\n",
      "Iteration 5860 : Loss 2960.5240\n",
      "Iteration 5870 : Loss 2960.4623\n",
      "Iteration 5880 : Loss 2960.4011\n",
      "Iteration 5890 : Loss 2960.3404\n",
      "Iteration 5900 : Loss 2960.2801\n",
      "Iteration 5910 : Loss 2960.2202\n",
      "Iteration 5920 : Loss 2960.1608\n",
      "Iteration 5930 : Loss 2960.1018\n",
      "Iteration 5940 : Loss 2960.0433\n",
      "Iteration 5950 : Loss 2959.9852\n",
      "Iteration 5960 : Loss 2959.9275\n",
      "Iteration 5970 : Loss 2959.8703\n",
      "Iteration 5980 : Loss 2959.8134\n",
      "Iteration 5990 : Loss 2959.7570\n",
      "Iteration 6000 : Loss 2959.7010\n",
      "Iteration 6010 : Loss 2959.6454\n",
      "Iteration 6020 : Loss 2959.5902\n",
      "Iteration 6030 : Loss 2959.5354\n",
      "Iteration 6040 : Loss 2959.4810\n",
      "Iteration 6050 : Loss 2959.4270\n",
      "Iteration 6060 : Loss 2959.3734\n",
      "Iteration 6070 : Loss 2959.3202\n",
      "Iteration 6080 : Loss 2959.2674\n",
      "Iteration 6090 : Loss 2959.2150\n",
      "Iteration 6100 : Loss 2959.1629\n",
      "Iteration 6110 : Loss 2959.1112\n",
      "Iteration 6120 : Loss 2959.0599\n",
      "Iteration 6130 : Loss 2959.0090\n",
      "Iteration 6140 : Loss 2958.9584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6150 : Loss 2958.9082\n",
      "Iteration 6160 : Loss 2958.8583\n",
      "Iteration 6170 : Loss 2958.8089\n",
      "Iteration 6180 : Loss 2958.7597\n",
      "Iteration 6190 : Loss 2958.7110\n",
      "Iteration 6200 : Loss 2958.6625\n",
      "Iteration 6210 : Loss 2958.6144\n",
      "Iteration 6220 : Loss 2958.5667\n",
      "Iteration 6230 : Loss 2958.5193\n",
      "Iteration 6240 : Loss 2958.4723\n",
      "Iteration 6250 : Loss 2958.4255\n",
      "Iteration 6260 : Loss 2958.3792\n",
      "Iteration 6270 : Loss 2958.3331\n",
      "Iteration 6280 : Loss 2958.2874\n",
      "Iteration 6290 : Loss 2958.2420\n",
      "Iteration 6300 : Loss 2958.1969\n",
      "Iteration 6310 : Loss 2958.1521\n",
      "Iteration 6320 : Loss 2958.1077\n",
      "Iteration 6330 : Loss 2958.0636\n",
      "Iteration 6340 : Loss 2958.0198\n",
      "Iteration 6350 : Loss 2957.9762\n",
      "Iteration 6360 : Loss 2957.9330\n",
      "Iteration 6370 : Loss 2957.8901\n",
      "Iteration 6380 : Loss 2957.8476\n",
      "Iteration 6390 : Loss 2957.8053\n",
      "Iteration 6400 : Loss 2957.7633\n",
      "Iteration 6410 : Loss 2957.7216\n",
      "Iteration 6420 : Loss 2957.6802\n",
      "Iteration 6430 : Loss 2957.6390\n",
      "Iteration 6440 : Loss 2957.5982\n",
      "Iteration 6450 : Loss 2957.5577\n",
      "Iteration 6460 : Loss 2957.5174\n",
      "Iteration 6470 : Loss 2957.4774\n",
      "Iteration 6480 : Loss 2957.4377\n",
      "Iteration 6490 : Loss 2957.3983\n",
      "Iteration 6500 : Loss 2957.3591\n",
      "Iteration 6510 : Loss 2957.3202\n",
      "Iteration 6520 : Loss 2957.2816\n",
      "Iteration 6530 : Loss 2957.2433\n",
      "Iteration 6540 : Loss 2957.2052\n",
      "Iteration 6550 : Loss 2957.1674\n",
      "Iteration 6560 : Loss 2957.1298\n",
      "Iteration 6570 : Loss 2957.0925\n",
      "Iteration 6580 : Loss 2957.0555\n",
      "Iteration 6590 : Loss 2957.0187\n",
      "Iteration 6600 : Loss 2956.9822\n",
      "Iteration 6610 : Loss 2956.9459\n",
      "Iteration 6620 : Loss 2956.9099\n",
      "Iteration 6630 : Loss 2956.8741\n",
      "Iteration 6640 : Loss 2956.8385\n",
      "Iteration 6650 : Loss 2956.8032\n",
      "Iteration 6660 : Loss 2956.7682\n",
      "Iteration 6670 : Loss 2956.7334\n",
      "Iteration 6680 : Loss 2956.6988\n",
      "Iteration 6690 : Loss 2956.6644\n",
      "Iteration 6700 : Loss 2956.6303\n",
      "Iteration 6710 : Loss 2956.5964\n",
      "Iteration 6720 : Loss 2956.5628\n",
      "Iteration 6730 : Loss 2956.5294\n",
      "Iteration 6740 : Loss 2956.4962\n",
      "Iteration 6750 : Loss 2956.4632\n",
      "Iteration 6760 : Loss 2956.4304\n",
      "Iteration 6770 : Loss 2956.3979\n",
      "Iteration 6780 : Loss 2956.3656\n",
      "Iteration 6790 : Loss 2956.3335\n",
      "Iteration 6800 : Loss 2956.3016\n",
      "Iteration 6810 : Loss 2956.2700\n",
      "Iteration 6820 : Loss 2956.2385\n",
      "Iteration 6830 : Loss 2956.2073\n",
      "Iteration 6840 : Loss 2956.1762\n",
      "Iteration 6850 : Loss 2956.1454\n",
      "Iteration 6860 : Loss 2956.1148\n",
      "Iteration 6870 : Loss 2956.0844\n",
      "Iteration 6880 : Loss 2956.0541\n",
      "Iteration 6890 : Loss 2956.0241\n",
      "Iteration 6900 : Loss 2955.9943\n",
      "Iteration 6910 : Loss 2955.9647\n",
      "Iteration 6920 : Loss 2955.9353\n",
      "Iteration 6930 : Loss 2955.9060\n",
      "Iteration 6940 : Loss 2955.8770\n",
      "Iteration 6950 : Loss 2955.8482\n",
      "Iteration 6960 : Loss 2955.8195\n",
      "Iteration 6970 : Loss 2955.7910\n",
      "Iteration 6980 : Loss 2955.7628\n",
      "Iteration 6990 : Loss 2955.7347\n",
      "Iteration 7000 : Loss 2955.7068\n",
      "Iteration 7010 : Loss 2955.6790\n",
      "Iteration 7020 : Loss 2955.6515\n",
      "Iteration 7030 : Loss 2955.6241\n",
      "Iteration 7040 : Loss 2955.5969\n",
      "Iteration 7050 : Loss 2955.5699\n",
      "Iteration 7060 : Loss 2955.5431\n",
      "Iteration 7070 : Loss 2955.5164\n",
      "Iteration 7080 : Loss 2955.4899\n",
      "Iteration 7090 : Loss 2955.4636\n",
      "Iteration 7100 : Loss 2955.4375\n",
      "Iteration 7110 : Loss 2955.4115\n",
      "Iteration 7120 : Loss 2955.3857\n",
      "Iteration 7130 : Loss 2955.3601\n",
      "Iteration 7140 : Loss 2955.3346\n",
      "Iteration 7150 : Loss 2955.3093\n",
      "Iteration 7160 : Loss 2955.2841\n",
      "Iteration 7170 : Loss 2955.2591\n",
      "Iteration 7180 : Loss 2955.2343\n",
      "Iteration 7190 : Loss 2955.2096\n",
      "Iteration 7200 : Loss 2955.1851\n",
      "Iteration 7210 : Loss 2955.1607\n",
      "Iteration 7220 : Loss 2955.1365\n",
      "Iteration 7230 : Loss 2955.1125\n",
      "Iteration 7240 : Loss 2955.0886\n",
      "Iteration 7250 : Loss 2955.0649\n",
      "Iteration 7260 : Loss 2955.0413\n",
      "Iteration 7270 : Loss 2955.0178\n",
      "Iteration 7280 : Loss 2954.9945\n",
      "Iteration 7290 : Loss 2954.9714\n",
      "Iteration 7300 : Loss 2954.9484\n",
      "Iteration 7310 : Loss 2954.9255\n",
      "Iteration 7320 : Loss 2954.9028\n",
      "Iteration 7330 : Loss 2954.8802\n",
      "Iteration 7340 : Loss 2954.8578\n",
      "Iteration 7350 : Loss 2954.8355\n",
      "Iteration 7360 : Loss 2954.8134\n",
      "Iteration 7370 : Loss 2954.7913\n",
      "Iteration 7380 : Loss 2954.7695\n",
      "Iteration 7390 : Loss 2954.7477\n",
      "Iteration 7400 : Loss 2954.7261\n",
      "Iteration 7410 : Loss 2954.7046\n",
      "Iteration 7420 : Loss 2954.6833\n",
      "Iteration 7430 : Loss 2954.6621\n",
      "Iteration 7440 : Loss 2954.6410\n",
      "Iteration 7450 : Loss 2954.6201\n",
      "Iteration 7460 : Loss 2954.5993\n",
      "Iteration 7470 : Loss 2954.5786\n",
      "Iteration 7480 : Loss 2954.5580\n",
      "Iteration 7490 : Loss 2954.5376\n",
      "Iteration 7500 : Loss 2954.5173\n",
      "Iteration 7510 : Loss 2954.4971\n",
      "Iteration 7520 : Loss 2954.4770\n",
      "Iteration 7530 : Loss 2954.4571\n",
      "Iteration 7540 : Loss 2954.4373\n",
      "Iteration 7550 : Loss 2954.4176\n",
      "Iteration 7560 : Loss 2954.3980\n",
      "Iteration 7570 : Loss 2954.3785\n",
      "Iteration 7580 : Loss 2954.3592\n",
      "Iteration 7590 : Loss 2954.3400\n",
      "Iteration 7600 : Loss 2954.3209\n",
      "Iteration 7610 : Loss 2954.3019\n",
      "Iteration 7620 : Loss 2954.2830\n",
      "Iteration 7630 : Loss 2954.2642\n",
      "Iteration 7640 : Loss 2954.2456\n",
      "Iteration 7650 : Loss 2954.2271\n",
      "Iteration 7660 : Loss 2954.2086\n",
      "Iteration 7670 : Loss 2954.1903\n",
      "Iteration 7680 : Loss 2954.1721\n",
      "Iteration 7690 : Loss 2954.1540\n",
      "Iteration 7700 : Loss 2954.1360\n",
      "Iteration 7710 : Loss 2954.1181\n",
      "Iteration 7720 : Loss 2954.1003\n",
      "Iteration 7730 : Loss 2954.0827\n",
      "Iteration 7740 : Loss 2954.0651\n",
      "Iteration 7750 : Loss 2954.0476\n",
      "Iteration 7760 : Loss 2954.0303\n",
      "Iteration 7770 : Loss 2954.0130\n",
      "Iteration 7780 : Loss 2953.9959\n",
      "Iteration 7790 : Loss 2953.9788\n",
      "Iteration 7800 : Loss 2953.9618\n",
      "Iteration 7810 : Loss 2953.9450\n",
      "Iteration 7820 : Loss 2953.9282\n",
      "Iteration 7830 : Loss 2953.9116\n",
      "Iteration 7840 : Loss 2953.8950\n",
      "Iteration 7850 : Loss 2953.8785\n",
      "Iteration 7860 : Loss 2953.8621\n",
      "Iteration 7870 : Loss 2953.8459\n",
      "Iteration 7880 : Loss 2953.8297\n",
      "Iteration 7890 : Loss 2953.8136\n",
      "Iteration 7900 : Loss 2953.7976\n",
      "Iteration 7910 : Loss 2953.7817\n",
      "Iteration 7920 : Loss 2953.7659\n",
      "Iteration 7930 : Loss 2953.7501\n",
      "Iteration 7940 : Loss 2953.7345\n",
      "Iteration 7950 : Loss 2953.7190\n",
      "Iteration 7960 : Loss 2953.7035\n",
      "Iteration 7970 : Loss 2953.6881\n",
      "Iteration 7980 : Loss 2953.6729\n",
      "Iteration 7990 : Loss 2953.6577\n",
      "Iteration 8000 : Loss 2953.6426\n",
      "Iteration 8010 : Loss 2953.6275\n",
      "Iteration 8020 : Loss 2953.6126\n",
      "Iteration 8030 : Loss 2953.5978\n",
      "Iteration 8040 : Loss 2953.5830\n",
      "Iteration 8050 : Loss 2953.5683\n",
      "Iteration 8060 : Loss 2953.5537\n",
      "Iteration 8070 : Loss 2953.5392\n",
      "Iteration 8080 : Loss 2953.5247\n",
      "Iteration 8090 : Loss 2953.5104\n",
      "Iteration 8100 : Loss 2953.4961\n",
      "Iteration 8110 : Loss 2953.4819\n",
      "Iteration 8120 : Loss 2953.4678\n",
      "Iteration 8130 : Loss 2953.4538\n",
      "Iteration 8140 : Loss 2953.4398\n",
      "Iteration 8150 : Loss 2953.4259\n",
      "Iteration 8160 : Loss 2953.4121\n",
      "Iteration 8170 : Loss 2953.3984\n",
      "Iteration 8180 : Loss 2953.3847\n",
      "Iteration 8190 : Loss 2953.3711\n",
      "Iteration 8200 : Loss 2953.3576\n",
      "Iteration 8210 : Loss 2953.3442\n",
      "Iteration 8220 : Loss 2953.3308\n",
      "Iteration 8230 : Loss 2953.3175\n",
      "Iteration 8240 : Loss 2953.3043\n",
      "Iteration 8250 : Loss 2953.2912\n",
      "Iteration 8260 : Loss 2953.2781\n",
      "Iteration 8270 : Loss 2953.2651\n",
      "Iteration 8280 : Loss 2953.2522\n",
      "Iteration 8290 : Loss 2953.2393\n",
      "Iteration 8300 : Loss 2953.2265\n",
      "Iteration 8310 : Loss 2953.2138\n",
      "Iteration 8320 : Loss 2953.2012\n",
      "Iteration 8330 : Loss 2953.1886\n",
      "Iteration 8340 : Loss 2953.1761\n",
      "Iteration 8350 : Loss 2953.1636\n",
      "Iteration 8360 : Loss 2953.1512\n",
      "Iteration 8370 : Loss 2953.1389\n",
      "Iteration 8380 : Loss 2953.1267\n",
      "Iteration 8390 : Loss 2953.1145\n",
      "Iteration 8400 : Loss 2953.1023\n",
      "Iteration 8410 : Loss 2953.0903\n",
      "Iteration 8420 : Loss 2953.0783\n",
      "Iteration 8430 : Loss 2953.0664\n",
      "Iteration 8440 : Loss 2953.0545\n",
      "Iteration 8450 : Loss 2953.0427\n",
      "Iteration 8460 : Loss 2953.0309\n",
      "Iteration 8470 : Loss 2953.0193\n",
      "Iteration 8480 : Loss 2953.0076\n",
      "Iteration 8490 : Loss 2952.9961\n",
      "Iteration 8500 : Loss 2952.9846\n",
      "Iteration 8510 : Loss 2952.9731\n",
      "Iteration 8520 : Loss 2952.9617\n",
      "Iteration 8530 : Loss 2952.9504\n",
      "Iteration 8540 : Loss 2952.9391\n",
      "Iteration 8550 : Loss 2952.9279\n",
      "Iteration 8560 : Loss 2952.9168\n",
      "Iteration 8570 : Loss 2952.9057\n",
      "Iteration 8580 : Loss 2952.8947\n",
      "Iteration 8590 : Loss 2952.8837\n",
      "Iteration 8600 : Loss 2952.8728\n",
      "Iteration 8610 : Loss 2952.8619\n",
      "Iteration 8620 : Loss 2952.8511\n",
      "Iteration 8630 : Loss 2952.8403\n",
      "Iteration 8640 : Loss 2952.8296\n",
      "Iteration 8650 : Loss 2952.8190\n",
      "Iteration 8660 : Loss 2952.8084\n",
      "Iteration 8670 : Loss 2952.7978\n",
      "Iteration 8680 : Loss 2952.7873\n",
      "Iteration 8690 : Loss 2952.7769\n",
      "Iteration 8700 : Loss 2952.7665\n",
      "Iteration 8710 : Loss 2952.7562\n",
      "Iteration 8720 : Loss 2952.7459\n",
      "Iteration 8730 : Loss 2952.7357\n",
      "Iteration 8740 : Loss 2952.7255\n",
      "Iteration 8750 : Loss 2952.7154\n",
      "Iteration 8760 : Loss 2952.7053\n",
      "Iteration 8770 : Loss 2952.6953\n",
      "Iteration 8780 : Loss 2952.6853\n",
      "Iteration 8790 : Loss 2952.6753\n",
      "Iteration 8800 : Loss 2952.6655\n",
      "Iteration 8810 : Loss 2952.6556\n",
      "Iteration 8820 : Loss 2952.6458\n",
      "Iteration 8830 : Loss 2952.6361\n",
      "Iteration 8840 : Loss 2952.6264\n",
      "Iteration 8850 : Loss 2952.6168\n",
      "Iteration 8860 : Loss 2952.6072\n",
      "Iteration 8870 : Loss 2952.5976\n",
      "Iteration 8880 : Loss 2952.5881\n",
      "Iteration 8890 : Loss 2952.5787\n",
      "Iteration 8900 : Loss 2952.5693\n",
      "Iteration 8910 : Loss 2952.5599\n",
      "Iteration 8920 : Loss 2952.5506\n",
      "Iteration 8930 : Loss 2952.5413\n",
      "Iteration 8940 : Loss 2952.5320\n",
      "Iteration 8950 : Loss 2952.5229\n",
      "Iteration 8960 : Loss 2952.5137\n",
      "Iteration 8970 : Loss 2952.5046\n",
      "Iteration 8980 : Loss 2952.4955\n",
      "Iteration 8990 : Loss 2952.4865\n",
      "Iteration 9000 : Loss 2952.4775\n",
      "Iteration 9010 : Loss 2952.4686\n",
      "Iteration 9020 : Loss 2952.4597\n",
      "Iteration 9030 : Loss 2952.4509\n",
      "Iteration 9040 : Loss 2952.4421\n",
      "Iteration 9050 : Loss 2952.4333\n",
      "Iteration 9060 : Loss 2952.4246\n",
      "Iteration 9070 : Loss 2952.4159\n",
      "Iteration 9080 : Loss 2952.4072\n",
      "Iteration 9090 : Loss 2952.3986\n",
      "Iteration 9100 : Loss 2952.3901\n",
      "Iteration 9110 : Loss 2952.3815\n",
      "Iteration 9120 : Loss 2952.3730\n",
      "Iteration 9130 : Loss 2952.3646\n",
      "Iteration 9140 : Loss 2952.3562\n",
      "Iteration 9150 : Loss 2952.3478\n",
      "Iteration 9160 : Loss 2952.3395\n",
      "Iteration 9170 : Loss 2952.3312\n",
      "Iteration 9180 : Loss 2952.3229\n",
      "Iteration 9190 : Loss 2952.3147\n",
      "Iteration 9200 : Loss 2952.3065\n",
      "Iteration 9210 : Loss 2952.2984\n",
      "Iteration 9220 : Loss 2952.2902\n",
      "Iteration 9230 : Loss 2952.2822\n",
      "Iteration 9240 : Loss 2952.2741\n",
      "Iteration 9250 : Loss 2952.2661\n",
      "Iteration 9260 : Loss 2952.2582\n",
      "Iteration 9270 : Loss 2952.2502\n",
      "Iteration 9280 : Loss 2952.2423\n",
      "Iteration 9290 : Loss 2952.2345\n",
      "Iteration 9300 : Loss 2952.2266\n",
      "Iteration 9310 : Loss 2952.2188\n",
      "Iteration 9320 : Loss 2952.2111\n",
      "Iteration 9330 : Loss 2952.2033\n",
      "Iteration 9340 : Loss 2952.1956\n",
      "Iteration 9350 : Loss 2952.1880\n",
      "Iteration 9360 : Loss 2952.1804\n",
      "Iteration 9370 : Loss 2952.1728\n",
      "Iteration 9380 : Loss 2952.1652\n",
      "Iteration 9390 : Loss 2952.1577\n",
      "Iteration 9400 : Loss 2952.1502\n",
      "Iteration 9410 : Loss 2952.1427\n",
      "Iteration 9420 : Loss 2952.1353\n",
      "Iteration 9430 : Loss 2952.1279\n",
      "Iteration 9440 : Loss 2952.1205\n",
      "Iteration 9450 : Loss 2952.1132\n",
      "Iteration 9460 : Loss 2952.1059\n",
      "Iteration 9470 : Loss 2952.0986\n",
      "Iteration 9480 : Loss 2952.0913\n",
      "Iteration 9490 : Loss 2952.0841\n",
      "Iteration 9500 : Loss 2952.0769\n",
      "Iteration 9510 : Loss 2952.0698\n",
      "Iteration 9520 : Loss 2952.0627\n",
      "Iteration 9530 : Loss 2952.0556\n",
      "Iteration 9540 : Loss 2952.0485\n",
      "Iteration 9550 : Loss 2952.0415\n",
      "Iteration 9560 : Loss 2952.0345\n",
      "Iteration 9570 : Loss 2952.0275\n",
      "Iteration 9580 : Loss 2952.0205\n",
      "Iteration 9590 : Loss 2952.0136\n",
      "Iteration 9600 : Loss 2952.0067\n",
      "Iteration 9610 : Loss 2951.9998\n",
      "Iteration 9620 : Loss 2951.9930\n",
      "Iteration 9630 : Loss 2951.9862\n",
      "Iteration 9640 : Loss 2951.9794\n",
      "Iteration 9650 : Loss 2951.9727\n",
      "Iteration 9660 : Loss 2951.9659\n",
      "Iteration 9670 : Loss 2951.9592\n",
      "Iteration 9680 : Loss 2951.9525\n",
      "Iteration 9690 : Loss 2951.9459\n",
      "Iteration 9700 : Loss 2951.9393\n",
      "Iteration 9710 : Loss 2951.9327\n",
      "Iteration 9720 : Loss 2951.9261\n",
      "Iteration 9730 : Loss 2951.9196\n",
      "Iteration 9740 : Loss 2951.9131\n",
      "Iteration 9750 : Loss 2951.9066\n",
      "Iteration 9760 : Loss 2951.9001\n",
      "Iteration 9770 : Loss 2951.8937\n",
      "Iteration 9780 : Loss 2951.8872\n",
      "Iteration 9790 : Loss 2951.8808\n",
      "Iteration 9800 : Loss 2951.8745\n",
      "Iteration 9810 : Loss 2951.8681\n",
      "Iteration 9820 : Loss 2951.8618\n",
      "Iteration 9830 : Loss 2951.8555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9840 : Loss 2951.8493\n",
      "Iteration 9850 : Loss 2951.8430\n",
      "Iteration 9860 : Loss 2951.8368\n",
      "Iteration 9870 : Loss 2951.8306\n",
      "Iteration 9880 : Loss 2951.8244\n",
      "Iteration 9890 : Loss 2951.8183\n",
      "Iteration 9900 : Loss 2951.8122\n",
      "Iteration 9910 : Loss 2951.8060\n",
      "Iteration 9920 : Loss 2951.8000\n",
      "Iteration 9930 : Loss 2951.7939\n",
      "Iteration 9940 : Loss 2951.7879\n",
      "Iteration 9950 : Loss 2951.7819\n",
      "Iteration 9960 : Loss 2951.7759\n",
      "Iteration 9970 : Loss 2951.7699\n",
      "Iteration 9980 : Loss 2951.7640\n",
      "Iteration 9990 : Loss 2951.7580\n",
      "Iteration 10000 : Loss 2951.7521\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d2b36",
   "metadata": {},
   "source": [
    "## (10) test 데이터에 대한 성능 확인하기\n",
    "\n",
    "test 데이터에 대한 성능을 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "219d6bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2824.501093201547"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c2d43",
   "metadata": {},
   "source": [
    "## (11) 정답 데이터와 예측한 데이터 시각화하기\n",
    "\n",
    "x축에는 X 데이터의 첫 번째 컬럼을, y축에는 정답인 target 데이터를 넣어서 모델이 예측한 데이터를 시각화해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d716eeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_test[:, 0], y_test, label=\"true\")\n",
    "plt.scatter(X_test[:, 0], prediction, label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a16033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
