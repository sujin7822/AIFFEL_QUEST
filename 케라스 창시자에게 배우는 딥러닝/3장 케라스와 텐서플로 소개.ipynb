{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e400fe",
   "metadata": {},
   "source": [
    "3.1 텐서플로란?\n",
    "\n",
    "3.2 케라스란?\n",
    "\n",
    "3.3 케라스와 텐서플로의 간략한 역사\n",
    "\n",
    "3.4 딥러닝 작업 환경 설정하기\n",
    "\n",
    "3.5 텐서플로 시작하기\n",
    "\n",
    "3.6 신경망의 구조 : 핵심 Keras API 이해하기\n",
    "\n",
    "3.7 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400fadf",
   "metadata": {},
   "source": [
    "3.1 텐서플로란?\n",
    "- 구글에서 만든 파이썬 기반의 무료 오픈 소스 머신 러닝 플랫폼\n",
    "- 넘파이와 매우 비슷하게 텐서플로의 핵심 목적은 엔지니어와 연구자가 수치 텐서에 대한 수학적 표현을 적용할 수 있도록 하는 것\n",
    "- 하지만, 텐서플로는 다음과 같은 넘파이의 기능을 넘어선다.\n",
    "    \n",
    "    - 미분 가능한 어떤 표현식에 대해서도 자동으로 그레이디언트를 계산할 수 있으므로 머신 러닝에 매우 적합\n",
    "    - CPU 뿐만 아니라 고도로 병렬화된 하드웨어 가속기은 GPU, TPU에서도 실행 o\n",
    "    - 텐서플로에서 정의한 계산은 여러 머신에 쉽게 분산시킬 수 o\n",
    "    - 텐서플로 프로그램은 C++, 자바스크립트 .. 다른런타임에 맞게 변환 할 수 o => 따라서 텐서플로 애플리케이션을 실전 환경에 쉽게 배포 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c1a13",
   "metadata": {},
   "source": [
    "3.2 케라스란?\n",
    "- 텐서플로 위에 구축된 파이썬용 딥러닝 API로 어떤 종류의 딥러닝 모델도 쉽게 만들고 훈련할 수 있는 방법 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0097d7",
   "metadata": {},
   "source": [
    "3.3 케라스와 텐서플로의 간략한 역사\n",
    "- 케라스는 텐서플로보다 먼저 나왔다. 케라스가 텐서플로 위에 구축된 것인데 어떻게 텐서플로보다 먼저 나왔을까?\n",
    "\n",
    "    ㄴ 케라스는 원래 씨아노를 위한 라이브러리 였음\n",
    "    \n",
    "    \n",
    "- 텐서플로가 기술적으로 성숙한 수준에 도달한 후 케라스의 기본 백엔드가 되었다. 그후 케라스에 새로운 백엔드 옵션이 추가되었지만(CNTK, MXNet), 현재까지는 케라스는 텐서플로를 사용하는 단일 백엔드 API가 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcad1f4",
   "metadata": {},
   "source": [
    "3.4 딥러닝 작업 환경 설정\n",
    "1. NVIDIA GPU 구입하기\n",
    "2. 구글 클라우드나 AWS EC2의 GPU 인스턴스 사용\n",
    "3. 구글이 제공하는 노트북 서비스인 코랩의 무료 GPU 런타임 사용하기 => 우린 주로 이걸 사용..\n",
    "\n",
    "### 주피터 노트북 : 권장하는 딥러닝 실험 도구\n",
    "### 코랩 사용하기\n",
    "\n",
    "**- 코랩 시작하기**\n",
    "\n",
    "**- pip로 패키지 설치하기**\n",
    "\n",
    "**- GPU 런타임 사용하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ec095",
   "metadata": {},
   "source": [
    "3.5 텐서플로 시작하기\n",
    "\n",
    "### 1. 상수 텐서와 변수\n",
    "모두 1인 텐서\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "x = tf.ones(shape=(2, 1))\n",
    "print(x)\n",
    "```\n",
    "모두 0인 텐서\n",
    "\n",
    "```\n",
    "x = tf.zeros(shape=(2, 1))\n",
    "print(x)\n",
    "\n",
    "```\n",
    "랜덤 텐서\n",
    "```\n",
    "x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.)\n",
    "print(x)\n",
    "```\n",
    "넘파이 배열에 값 할당\n",
    "```\n",
    "import numpy as np\n",
    "x = np.ones(shape=(2, 2))\n",
    "x[0, 0] = 0.\n",
    "```\n",
    "넘파이와 달리 텐서플로는 텐서에 값을 할당하지 못한다.\n",
    "```\n",
    "x = tf.ones(shape=(2,2))\n",
    "x[0,0] = 0.\n",
    "```\n",
    "ㄴ이렇게 하면 오류 발생\n",
    "\n",
    "해결방안 : 텐서플로 변수 만들기 (assign 사용)\n",
    "\n",
    "```\n",
    "v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))\n",
    "print(v)\n",
    "```\n",
    "텐서플로 변수에 값 할당\n",
    "```\n",
    "v.assign(tf.ones((3, 1)))\n",
    "```\n",
    "변수 일부에 값 할당하기\n",
    "```\n",
    "v[0, 0].assign(3.)\n",
    "```\n",
    "비슷하게 assign_add()와 assign_sub() 사용하기\n",
    "```\n",
    "v.assign_add(tf.ones((3, 1)))\n",
    "```\n",
    "### 2. 텐서 연산 : 텐서플로에서 수학 계산하기\n",
    "기본적인 수학 연산\n",
    "```\n",
    "a = tf.ones((2, 2))\n",
    "b = tf.square(a)\n",
    "c = tf.sqrt(a)\n",
    "d = b + c\n",
    "e = tf.matmul(a, b)\n",
    "e *= d\n",
    "```\n",
    "### 3. GradientTape API 다시 살펴보기\n",
    "GradientTape 사용하기(입력 텐서가 텐서플로 변수인 경우)\n",
    "```\n",
    "input_var = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:\n",
    "   result = tf.square(input_var)\n",
    "gradient = tape.gradient(result, input_var)\n",
    "```\n",
    "상수 입력 텐서와 함께 GradientTape 사용하기 (입력 텐서가 상수 텐서의 경우tape.watch() 호출하기)\n",
    "```\n",
    "input_const = tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "   tape.watch(input_const)\n",
    "   result = tf.square(input_const)\n",
    "gradient = tape.gradient(result, input_const)\n",
    "```\n",
    "그레이디언트 테이프를 중첩하여 이계도 그레이디언트를 계산하기 (like, 미분에 미분)\n",
    "```\n",
    "time = tf.Variable(0.)\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as inner_tape:\n",
    "        position =  4.9 * time ** 2\n",
    "    speed = inner_tape.gradient(position, time)\n",
    "acceleration = outer_tape.gradient(speed, time)\n",
    "```\n",
    "### 4. 엔드-투-엔드 예제 : 텐서플로 선형 분류기\n",
    "선형 분류기의 변수 만들기\n",
    "```\n",
    "input_dim = 2\n",
    "output_dim = 1 # (0에 가까우면 class 0으로 아니면 1로 .. 이렇게 1차원적)\n",
    "W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))\n",
    "b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))\n",
    "```\n",
    "정방향 패스 함수\n",
    "```\n",
    "def model(inputs):\n",
    "    return tf.matmul(inputs, W) + b\n",
    "```\n",
    "평균 제곱 오차 손실 함수\n",
    "```\n",
    "def square_loss(targets, predictions):\n",
    "    per_sample_losses = tf.square(targets - predictions)\n",
    "    return tf.reduce_mean(per_sample_losses)\n",
    "```\n",
    "훈련 스텝 함수\n",
    "```\n",
    "learning_rate = 0.1\n",
    "\n",
    "def training_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = square_loss(targets, predictions)\n",
    "    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])\n",
    "    W.assign_sub(grad_loss_wrt_W * learning_rate)\n",
    "    b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
    "    return loss\n",
    "```\n",
    "배치 훈련 루프\n",
    "```\n",
    "for step in range(40):\n",
    "    loss = training_step(inputs, targets)\n",
    "    print(f\"{step}번째 스텝의 손실: {loss:.4f}\")\n",
    "```\n",
    "그래프 그려보기\n",
    "```\n",
    "predictions = model(inputs)\n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)\n",
    "plt.show()\n",
    "```\n",
    "선형 분류된 직선 포함한 그래프 그려보기\n",
    "```\n",
    "x = np.linspace(-1, 4, 100)\n",
    "# 사실 100개의 x 축 좌표를 만들 필요 없이 시작과 종료 위치만 있어도 됩니다.\n",
    "# x = [-1, 4]\n",
    "y = - W[0] /  W[1] * x + (0.5 - b) / W[1]\n",
    "plt.plot(x, y, \"-r\")\n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a90c1a",
   "metadata": {},
   "source": [
    "3.6 신경망의 구조: 핵심 Keras API 이해하기\n",
    "### 1. 층: 딥러닝의 구성 요소\n",
    "Layer의 서브클래스로 구현한 Dense 층\n",
    "```\n",
    "from tensorflow import keras\n",
    "\n",
    "class SimpleDense(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, activation=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.W = self.add_weight(shape=(input_dim, self.units),\n",
    "                                 initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer=\"zeros\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = tf.matmul(inputs, self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y\n",
    "```\n",
    "사용해 보면,\n",
    "```\n",
    "my_dense = SimpleDense(units=32, activation=tf.nn.relu)\n",
    "input_tensor = tf.ones(shape=(2, 784))\n",
    "output_tensor = my_dense(input_tensor)\n",
    "print(output_tensor.shape)\n",
    "```\n",
    "(2, 32)\n",
    "\n",
    "자동 크기 추론: 동적으로 층 만들기\n",
    "```\n",
    "from tensorflow.keras import layers\n",
    "layer = layers.Dense(32, activation=\"relu\")\n",
    "```\n",
    "케라스의 경우 크기 호환성 걱정 필요 x 동적으로 할당해줌\n",
    "```\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "model = models.Sequential([\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(32)\n",
    "])\n",
    "```\n",
    "예)\n",
    "```\n",
    "model = keras.Sequential([\n",
    "    SimpleDense(32, activation=\"relu\"),\n",
    "    SimpleDense(64, activation=\"relu\"),\n",
    "    SimpleDense(32, activation=\"relu\"),\n",
    "    SimpleDense(10, activation=\"softmax\")\n",
    "])\n",
    "```\n",
    "### 2. 층에서 모델로\n",
    "모델 구조 정의\n",
    "### 3. “컴파일” 단계: 학습 과정 설정\n",
    "손실 함수, 옵티마이저, 측정 지표 선택해줘야함\n",
    "```\n",
    "model = keras.Sequential([keras.layers.Dense(1)]) # 모델 구조 정의\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=[\"accuracy\"])\n",
    "```\n",
    "다른 방법으로는\n",
    "```\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "```\n",
    "옵티마이저\n",
    "- SGD(모멘텀 선택 가능)\n",
    "- RMSprop\n",
    "- Adam\n",
    "- Adagrad\n",
    "손실 함수\n",
    "- CategoricalCrossentropy\n",
    "- SparseCategoricalCrossentropy\n",
    "- BinaryCrossentropy\n",
    "- MeanSquaredError\n",
    "- KLDivergence\n",
    "- CosineSimilarity\n",
    "측정 지표\n",
    "- CategoricalAccuracy\n",
    "- SparseCategoricalAccuracy\n",
    "- BinaryAccuracy\n",
    "- AUC\n",
    "- Precision\n",
    "- Recall\n",
    "### 4. 손실 함수 선택하기\n",
    "BinaryCrossentropy => **2개의 클래스가 있는 분류**\n",
    "CategoricalCrossentropy => **여러 개의 클래스가 있는 분류**\n",
    "\n",
    "### 5. fit() 메서드 이해하기\n",
    "넘파이 데이터로 fit() 메서드 호출하기\n",
    "```\n",
    "history = model.fit(\n",
    "    inputs,\n",
    "    targets,\n",
    "    epochs=5,\n",
    "    batch_size=128\n",
    ")\n",
    "```\n",
    "Epoch 1/5\n",
    "16/16 [==============================] - 1s 2ms/step - loss: 10.1885 - binary_accuracy: 0.0020\n",
    "Epoch 2/5\n",
    "16/16 [==============================] - 0s 2ms/step - loss: 9.8145 - binary_accuracy: 0.0020 \n",
    "Epoch 3/5\n",
    "16/16 [==============================] - 0s 2ms/step - loss: 9.4990 - binary_accuracy: 0.0020\n",
    "Epoch 4/5\n",
    "16/16 [==============================] - 0s 2ms/step - loss: 9.1918 - binary_accuracy: 0.0020\n",
    "Epoch 5/5\n",
    "16/16 [==============================] - 0s 1ms/step - loss: 8.8920 - binary_accuracy: 0.0020\n",
    "```\n",
    "history.history\n",
    "```\n",
    "{'loss': [10.188525199890137,\n",
    "  9.814451217651367,\n",
    "  9.498977661132812,\n",
    "  9.191787719726562,\n",
    "  8.891997337341309],\n",
    " 'binary_accuracy': [0.0020000000949949026,\n",
    "  0.0020000000949949026,\n",
    "  0.0020000000949949026,\n",
    "  0.0020000000949949026,\n",
    "  0.0020000000949949026]}\n",
    "### 6. 검증 데이터에서 손실과 측정 지표 모니터링하기\n",
    "validation_data 매개변수 사용하기\n",
    "\n",
    "**validation_data=(val_inputs, val_targets)** 사용하기\n",
    "```\n",
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "indices_permutation = np.random.permutation(len(inputs))\n",
    "shuffled_inputs = inputs[indices_permutation]\n",
    "shuffled_targets = targets[indices_permutation]\n",
    "\n",
    "num_validation_samples = int(0.3 * len(inputs))\n",
    "val_inputs = shuffled_inputs[:num_validation_samples]\n",
    "val_targets = shuffled_targets[:num_validation_samples]\n",
    "training_inputs = shuffled_inputs[num_validation_samples:]\n",
    "training_targets = shuffled_targets[num_validation_samples:]\n",
    "model.fit(\n",
    "    training_inputs,\n",
    "    training_targets,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs, val_targets)\n",
    ")\n",
    "```\n",
    "Epoch 1/5\n",
    "88/88 [==============================] - 1s 4ms/step - loss: 0.2450 - binary_accuracy: 0.9093 - val_loss: 0.0913 - val_binary_accuracy: 0.9850\n",
    "Epoch 2/5\n",
    "88/88 [==============================] - 0s 3ms/step - loss: 0.0680 - binary_accuracy: 0.9643 - val_loss: 0.0270 - val_binary_accuracy: 1.0000\n",
    "Epoch 3/5\n",
    "88/88 [==============================] - 0s 5ms/step - loss: 0.0779 - binary_accuracy: 0.9507 - val_loss: 0.0232 - val_binary_accuracy: 0.9967\n",
    "Epoch 4/5\n",
    "88/88 [==============================] - 0s 4ms/step - loss: 0.0765 - binary_accuracy: 0.9407 - val_loss: 0.0598 - val_binary_accuracy: 0.9883\n",
    "Epoch 5/5\n",
    "88/88 [==============================] - 0s 5ms/step - loss: 0.0638 - binary_accuracy: 0.9721 - val_loss: 0.0726 - val_binary_accuracy: 0.9533\n",
    "<keras.callbacks.History at 0x7f0787cb8bd0>\n",
    "### 7. 추론: 훈련한 모델 사용하기\n",
    "```\n",
    "predictions = model(new_inputs)\n",
    "```\n",
    "이건 call 메서드 호출하는 것\n",
    "\n",
    "=> 데이터 한 번에 처리하는 것 => 좋은 방법은 아님\n",
    "\n",
    "**predict()메서드를 사용하는 것이!**\n",
    "\n",
    "```\n",
    "predictions = model.predict(val_inputs, batch_size=128)\n",
    "print(predictions[:10])\n",
    "```\n",
    "5/5 [==============================] - 0s 3ms/step\n",
    "[[0.2212547 ]\n",
    " [0.46757108]\n",
    " [1.3064125 ]\n",
    " [0.36760795]\n",
    " [0.98762196]\n",
    " [1.2589831 ]\n",
    " [0.9364893 ]\n",
    " [0.2468108 ]\n",
    " [0.94096804]\n",
    " [0.4054407 ]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
